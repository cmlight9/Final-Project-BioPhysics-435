{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Base stacking prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDoVDZbdj-Df",
        "colab_type": "text"
      },
      "source": [
        "# Base stacking prediction\n",
        "\n",
        "This code uses a combination of different classifiers to predict base stacking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrhRgShykgxZ",
        "colab_type": "text"
      },
      "source": [
        "## Imports / function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCzABRHTQHvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All import statements\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import expon as sp_expon\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R42Wx0lDQFOz",
        "colab_type": "text"
      },
      "source": [
        "Define all neccessary functions for formatting the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An-TXDkDUAMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global variable \n",
        "NUMBER_CHEMICAL_SHIFT_TYPE = 19\n",
        "\n",
        "def one_hot(cs):\n",
        "  '''\n",
        "  This function encodes the resnames so that there are now 4 columns \n",
        "  corresponding to each possible resname.\n",
        "  '''\n",
        "  one_hot = pd.get_dummies(cs['resname'])\n",
        "  cs = cs.join(one_hot)\n",
        "  return(cs)\n",
        "\n",
        "def get_cs_all(cs_all, id = \"2KOC\"):\n",
        "  '''    \n",
        "    This function gets chemical shifts for a particular RNA. \n",
        "    Assumes each RNA has a unique id  \n",
        "  '''\n",
        "  return(cs_all[(cs_all.id == id)])\n",
        "\n",
        "def get_cs_residues(cs_i, resid, dummy = 0):\n",
        "  '''    \n",
        "    This function return an array containing the chemical shifts for a particular residues in an RNA.    \n",
        "  '''\n",
        "  cs_tmp = cs_i[(cs_i.resid == resid)].drop(['id', 'resid', 'resname', 'stacking', 'ADE', 'CYT', 'GUA', 'URA'], axis=1)\n",
        "  info_tmp = cs_i[(cs_i.resid == resid)]\n",
        "  if (cs_tmp.shape[0] != 1):\n",
        "     return(dummy*np.ones(shape=(1, NUMBER_CHEMICAL_SHIFT_TYPE)))\n",
        "  else:\n",
        "     return(cs_tmp.values)\n",
        "    \n",
        "def get_resnames(cs_i, resid, dummy = \"UNK\"):\n",
        "  '''    \n",
        "    This function returns the residue name for specified residue (resid)\n",
        "  '''\n",
        "  cs_tmp = cs_i[(cs_i.resid == resid)]  \n",
        "  if (cs_tmp.shape[0] != 1):\n",
        "     return(dummy)\n",
        "  else:\n",
        "     return(cs_tmp['resname'].values[0])\n",
        "\n",
        "def get_cs_features(cs_i, resid, neighbors=1):\n",
        "  '''    \n",
        "  This function chemical shifts and resnames for residue (resid) and its neighbors        \n",
        "\n",
        "  '''\n",
        "  cs = []\n",
        "  resnames = []\n",
        "  for i in range(resid-neighbors, resid+neighbors+1):\n",
        "    cs.append(get_cs_residues(cs_i, i))\n",
        "    resnames.append(get_resnames(cs_i, i))\n",
        "  return(resnames, np.array(cs))\n",
        "\n",
        "def get_columns_names(neighbors = 3, chemical_shift_types = 19):\n",
        "  '''\n",
        "    \n",
        "    Helper function that writes out the required column names\n",
        "    \n",
        "  '''\n",
        "\n",
        "  columns = ['id', 'resname', 'resid', 'stacking', 'ADE', 'CYT', 'GUA', 'URA']\n",
        "  for i in range(0, neighbors*chemical_shift_types):\n",
        "    columns.append(i)\n",
        "  return(columns)\n",
        "\n",
        "def write_out_resname(neighbors=1):\n",
        "  '''\n",
        "  \n",
        "    Helper function that writes out the column names associated resnames for a given residue and its neighbors\n",
        "    \n",
        "  '''  \n",
        "  colnames = []\n",
        "  for i in range(1-neighbors-1, neighbors+1):\n",
        "    if i < 0: \n",
        "      colnames.append('R%s'%i)\n",
        "    elif i > 0: \n",
        "      colnames.append('R+%s'%i)\n",
        "    else: \n",
        "      colnames.append('R')\n",
        "  return(colnames)    \n",
        "\n",
        "\n",
        "def get_cs_features_rna(cs, neighbors=1, retain = ['id', 'stacking', 'resid', 'ADE', 'CYT', 'GUA', 'URA']):\n",
        "  '''    \n",
        "    This function generates the complete required data frame an RNA    \n",
        "  '''\n",
        "  all_features = []\n",
        "  all_resnames = []\n",
        "  for resid in cs['resid'].unique():\n",
        "    resnames, features = get_cs_features(cs, resid, neighbors)\n",
        "    all_features.append(features.flatten())\n",
        "    all_resnames.append(resnames)\n",
        "\n",
        "  all_resnames = pd.DataFrame(all_resnames, dtype='object', columns = write_out_resname(neighbors))\n",
        "  all_features = pd.DataFrame(all_features, dtype='object')\n",
        "  info = pd.DataFrame(cs[retain].values, dtype='object', columns = retain)\n",
        "  return(pd.concat([info, all_resnames, all_features], axis=1))\n",
        "\n",
        "def create_training_testing(cs, leave_out = \"2KOC\", target_name = 'stacking', neighbors = 2, drop_names = ['id', 'stacking', 'resid']):\n",
        "  '''    \n",
        "    This function creates a training and testing set using leave one out    \n",
        "  '''\n",
        "  \n",
        "  # drop extraneous data  \n",
        "  drop_names = drop_names + list(write_out_resname(neighbors))  \n",
        "  \n",
        "  # does not contain leave_out\n",
        "  train = cs[(cs.id != leave_out)]\n",
        "  trainX = train.drop(drop_names, axis=1)\n",
        "  trainy = train[target_name]\n",
        " \n",
        "  # only contains leave_out\n",
        "  test = cs[(cs.id == leave_out)]\n",
        "  testX = test.drop(drop_names, axis=1)\n",
        "  testy = test[target_name]\n",
        "  \n",
        "  # return training and testing data\n",
        "  return(trainX.values, trainy.values, testX.values, testy.values)\n",
        "\n",
        "def get_cs_features_rna_all(cs, neighbors = 2):  \n",
        "  '''    \n",
        "    This [should] function generate a pandas dataframe containing training data for all RNAs\n",
        "    Each row in the data frame should contain the stacking and chemical shifts for given residue and neighbors in a given RNA.\n",
        "    Use the function above to write function\n",
        "    \n",
        "  '''  \n",
        "  # Start: your code\n",
        "  \n",
        "  cs_new = pd.DataFrame()\n",
        "  \n",
        "  for id in c.id.unique():\n",
        "    cs_id = get_cs_all(cs,id)\n",
        "    cs_new = pd.concat([cs_new,get_cs_features_rna(cs_id, neighbors)], axis = 0)\n",
        "  \n",
        "  \n",
        "  # End: your code\n",
        "  return(cs_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL8h4wIekcBQ",
        "colab_type": "text"
      },
      "source": [
        "## Load in and prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckh8IRYPPd6U",
        "colab_type": "text"
      },
      "source": [
        "Load in .csv file and create database from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjOoO6XgP1lM",
        "colab_type": "code",
        "outputId": "f1b2f6b4-71ac-402c-d352-6bd5c200c963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")   \n",
        "\n",
        "# load initial data\n",
        "url=\"https://drive.google.com/uc?id=1e-SHtWDtg4mD_th3_4Jmq9r1iiQC32wT\"\n",
        "s=requests.get(url).content\n",
        "c=pd.read_csv(io.StringIO(s.decode('utf-8')), sep=' ')\n",
        "print(\"[INFO]: loaded data\")\n",
        "# Drop extraneous columns that are unneeded for prediction\n",
        "c = c.drop(['Unnamed: 0','base_pairing', 'orientation', 'sugar_puckering', 'pseudoknot'], axis = 1)\n",
        "# Convert stacking column to numerical\n",
        "c['stacking'] = c['stacking'].map({'stack': 1, 'non-stack': 0})\n",
        "# One-hot encode the resname data\n",
        "c_new = one_hot(c)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: loaded data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2TeC7pflZNJ",
        "colab_type": "text"
      },
      "source": [
        "## Set up hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zni-peRC45fI",
        "colab_type": "text"
      },
      "source": [
        "Below, we initialize the hyperparameter space distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hILS7RroUlI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the parameter space distribution from which to get sample hyperparameters\n",
        "# in determing the optimal ones to use.\n",
        "min_size, max_size = 5, 100\n",
        "parameter_space_distribution = {\n",
        "    'hidden_layer_sizes': [(sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': sp_expon(scale=.01),\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "    'learning_rate_init': sp_expon(scale=.001),\n",
        "} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puEtNI6L5CY6",
        "colab_type": "text"
      },
      "source": [
        "We will go through the training process once with the first ID data left out. This is used to initialize the hyperparameters for running on the program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_W6a2RDUtc4",
        "colab_type": "code",
        "outputId": "bc7d09f9-e190-42c2-ade0-efce392846bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Initialize empty classification array to use for calculating mean F1-score later\n",
        "classification_array = []\n",
        "\n",
        "# The ideal number of neighbors found in a prior assignment was 4\n",
        "NEIGHBORS = 4\n",
        "\n",
        "# Initialize the ID array which will be used later to cycle through IDs.\n",
        "id_array = c.id.unique()\n",
        "\n",
        "# The code below is used to randomly select a subset of IDs to cycle through\n",
        "# in case the program needs some speeding up\n",
        "#np.random.shuffle(id_array)\n",
        "#id_array = id_array[0:50]\n",
        "id = id_array[0]\n",
        "\n",
        "# Get all features and ready the data for model fitting by adding neighbor CS columns\n",
        "cs_all = get_cs_features_rna_all(c_new, neighbors = NEIGHBORS)\n",
        "\n",
        "# Seperate data into training and testing set while leaving out one ID for testing\n",
        "trainX, trainy, testX, testy = create_training_testing(cs_all, leave_out = id, neighbors = NEIGHBORS)\n",
        "print(\"[INFO]: created training and testing data structures\")\n",
        "\n",
        "# setup scaler and scale the training and testing input data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(trainX)\n",
        "trainX_scaled = scaler.transform(trainX)\n",
        "testX_scaled = scaler.transform(testX)\n",
        "print(\"[INFO]: scaled the features\")\n",
        "\n",
        "# build a classifier - in this case a simple Multi-layer perceptron classifier\n",
        "clf = MLPClassifier(max_iter=100)\n",
        "\n",
        "# Random search for best hyperparameters\n",
        "n_iter_search = 6\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=parameter_space_distribution, n_iter=n_iter_search, cv=6, verbose = 1)\n",
        "random_search.fit(trainX_scaled, np.int_(trainy))\n",
        "print(\"[INFO]: hyperparameter search complete\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "Fitting 6 folds for each of 6 candidates, totalling 36 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  1.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO]: hyperparameter search complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae0b189i02B7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "89dcb19c-863c-4fc6-a61f-191d699d38d8"
      },
      "source": [
        "random_search.best_params_"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'tanh',\n",
              " 'alpha': 0.02300468536963886,\n",
              " 'hidden_layer_sizes': (77, 80, 18),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.00043915907775182283,\n",
              " 'solver': 'sgd'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVmm5XidYFav",
        "colab_type": "text"
      },
      "source": [
        "Parameters chosen from randommized searching were:\n",
        "```\n",
        "activation  = 'tanh',\n",
        "alpha = 0.014025693410201838,\n",
        "hidden_layer_sizes = (186, 153),\n",
        "learning_rate = 'constant',\n",
        "learning_rate_init = 0.006439317102679333,\n",
        "solver = 'lbfgs'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FZhVuUA5VnX",
        "colab_type": "text"
      },
      "source": [
        "## Loop through IDs to get average f1-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPqd-Lze6XXJ",
        "colab_type": "text"
      },
      "source": [
        "Now that we have set up the hyperparameters to use in our model, the next step is to cycle through leaving out different IDs and then average over the resulting accuracy f1-scores. For the classifiers, I round the results so that anything 0.5 or below becomes 0 and anything above 0.5 becomes 1.\n",
        "\n",
        "The basic process will work like this\n",
        "1. Set up the models\n",
        "2. Fit the models\n",
        "3. Use the test data to predict y values\n",
        "4. Combine models by averaging the y values\n",
        "5. Average weighted avg f1 scores for individual and combined models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIhjiTpEtDE7",
        "colab_type": "code",
        "outputId": "b2621270-ec18-41a6-93fb-9b76df754c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set up empty classification arrays\n",
        "mlp_array = []\n",
        "lda_array = []\n",
        "knc_array = []\n",
        "gnb_array = []\n",
        "dtc_array = []\n",
        "svc_array = []\n",
        "combined_array = []\n",
        "\n",
        "for id in id_array:\n",
        "  # Generate training and testing data from the dataset, leaving out one ID at a time\n",
        "  trainX, trainy, testX, testy = create_training_testing(cs_all, leave_out = id, neighbors = NEIGHBORS)\n",
        "  print(\"[INFO]: created training and testing data structures\")\n",
        "\n",
        "  # setup scaler and fit to the training/testing input data\n",
        "  scaler.fit(trainX)\n",
        "  trainX_scaled = scaler.transform(trainX)\n",
        "  testX_scaled = scaler.transform(testX)\n",
        "  print(\"[INFO]: scaled the features\")\n",
        "\n",
        "  # Fit the new set of data to the model based on the random search hyperparameters\n",
        "  mlp_clf = MLPClassifier(activation  = 'tanh',\n",
        "                          alpha = 0.014025693410201838,\n",
        "                          hidden_layer_sizes = (186, 153),\n",
        "                          learning_rate = 'constant',\n",
        "                          learning_rate_init = 0.006439317102679333,\n",
        "                          solver = 'lbfgs')\n",
        "  # Set up some other classifiers as well (using default values)\n",
        "  lda_clf = LinearDiscriminantAnalysis()\n",
        "  knc_clf = KNeighborsClassifier()\n",
        "  gnb_clf = GaussianNB()\n",
        "  dtc_clf = DecisionTreeClassifier()\n",
        "  svc_clf = SVC()\n",
        "\n",
        "  # Fit all of the classifier models.\n",
        "  mlp_clf.fit(trainX_scaled, np.int_(trainy))\n",
        "  lda_clf.fit(trainX_scaled, np.int_(trainy))\n",
        "  knc_clf.fit(trainX_scaled, np.int_(trainy))\n",
        "  gnb_clf.fit(trainX_scaled, np.int_(trainy))\n",
        "  dtc_clf.fit(trainX_scaled, np.int_(trainy))\n",
        "  svc_clf.fit(trainX_scaled, np.int_(trainy))\n",
        "\n",
        "  # predict y values based on model fits\n",
        "  ypred_lda = lda_clf.predict(testX_scaled)\n",
        "  ypred_knc = knc_clf.predict(testX_scaled)\n",
        "  ypred_gnb = gnb_clf.predict(testX_scaled)\n",
        "  ypred_dtc = dtc_clf.predict(testX_scaled)\n",
        "  ypred_svc = svc_clf.predict(testX_scaled)\n",
        "  ypred_mlp = mlp_clf.predict(testX_scaled)\n",
        "\n",
        "  # Combine the predictions by taking the mean of their values\n",
        "  ypred_combined = np.rint(np.mean([ypred_lda,ypred_knc, ypred_mlp, ypred_gnb,ypred_dtc,ypred_svc], axis=0))\n",
        "\n",
        "  # Record the weighted average f1 scores for the combined and individual classifiers.\n",
        "  y_true = np.int_(testy)\n",
        "  mlp_array.append(classification_report(np.int_(y_true),np.int_(ypred_mlp), output_dict=True).get('weighted avg').get('f1-score'))\n",
        "  lda_array.append(classification_report(np.int_(y_true),np.int_(ypred_lda), output_dict=True).get('weighted avg').get('f1-score'))\n",
        "  knc_array.append(classification_report(np.int_(y_true),np.int_(ypred_knc), output_dict=True).get('weighted avg').get('f1-score'))\n",
        "  gnb_array.append(classification_report(np.int_(y_true),np.int_(ypred_gnb), output_dict=True).get('weighted avg').get('f1-score'))\n",
        "  dtc_array.append(classification_report(np.int_(y_true),np.int_(ypred_dtc), output_dict=True).get('weighted avg').get('f1-score'))\n",
        "  svc_array.append(classification_report(np.int_(y_true),np.int_(ypred_svc), output_dict=True).get('weighted avg').get('f1-score'))\n",
        "  combined_array.append(classification_report(np.int_(y_true),np.int_(ypred_combined), output_dict=True).get('weighted avg').get('f1-score'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMmwcrFGnre4",
        "colab_type": "code",
        "outputId": "c31938d4-741e-49a4-b84e-a7c62b21ccc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# Displays the mean weighted avg f1-scores obtained from the run\n",
        "print(\"mlp:\")\n",
        "print(np.mean(mlp_array))\n",
        "print(\"lda:\")\n",
        "print(np.mean(lda_array))\n",
        "print(\"knc:\")\n",
        "print(np.mean(knc_array))\n",
        "print(\"gnb:\")\n",
        "print(np.mean(gnb_array))\n",
        "print(\"dtc:\")\n",
        "print(np.mean(dtc_array))\n",
        "print(\"svc:\")\n",
        "print(np.mean(svc_array))\n",
        "print(\"combined:\")\n",
        "print(np.mean(combined_array))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mlp:\n",
            "0.8269027059419968\n",
            "lda:\n",
            "0.839978796851995\n",
            "knc:\n",
            "0.8381739921249131\n",
            "gnb:\n",
            "0.7854625872269962\n",
            "dtc:\n",
            "0.8029526223118394\n",
            "svc:\n",
            "0.8116905631508463\n",
            "combined:\n",
            "0.839401346255776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx8qSlri5hTp",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion:\n",
        "\n",
        "This resulted in the combined model having the highest weighted avg f1-score for only one of the trials. So it does perform well compared to the baseline model, but unfortunately it is not consistent and none of the models made it to 0.9. One option could be to optimize the hyperparameters for the other classifiers besides the `MLPClassifier` since currently these use only the default values. But we should be careful not to overfit them either.\n",
        "\n",
        "Trial 1:\n",
        "\n",
        "mlp:\n",
        "0.8383482049711244\n",
        "\n",
        "lda:\n",
        "0.839978796851995\n",
        "\n",
        "knc:\n",
        "0.8381739921249131\n",
        "\n",
        "gnb:\n",
        "0.7854625872269962\n",
        "\n",
        "dtc:\n",
        "0.803306607289426\n",
        "\n",
        "svc:\n",
        "0.8116905631508463\n",
        "\n",
        "combined:\n",
        "0.8402191854224372\n",
        "\n",
        "Trial 2:\n",
        "\n",
        "mlp:\n",
        "0.8269027059419968\n",
        "\n",
        "lda:\n",
        "0.839978796851995\n",
        "\n",
        "knc:\n",
        "0.8381739921249131\n",
        "\n",
        "gnb:\n",
        "0.7854625872269962\n",
        "\n",
        "dtc:\n",
        "0.8029526223118394\n",
        "\n",
        "svc:\n",
        "0.8116905631508463\n",
        "\n",
        "combined:\n",
        "0.839401346255776\n",
        "\n",
        "Trial 3:\n",
        "\n",
        "mlp:\n",
        "0.8186848724954964\n",
        "\n",
        "lda:\n",
        "0.839978796851995\n",
        "\n",
        "knc:\n",
        "0.8381739921249131\n",
        "\n",
        "gnb:\n",
        "0.7854625872269962\n",
        "\n",
        "dtc:\n",
        "0.7973518253628668\n",
        "\n",
        "svc:\n",
        "0.8116905631508463\n",
        "\n",
        "combined:\n",
        "0.8374392223574743"
      ]
    }
  ]
}