{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SASA prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkZquMUmbqnJ",
        "colab_type": "text"
      },
      "source": [
        "#SASA Prediction Code\n",
        "\n",
        "This code trains a multi-layer perceptron regression to predict SASA values based on chemical shift data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70aa1Z2_f_CG",
        "colab_type": "text"
      },
      "source": [
        "## Setup the code with imports / function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC01xY6AHz_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All import statements\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import expon as sp_expon\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import sklearn\n",
        "\n",
        "from sklearn.linear_model import MultiTaskLasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giIa6SCaQLn7",
        "colab_type": "text"
      },
      "source": [
        "First, we define some necessary functions for formatting the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvGIgJNlQNuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global variable \n",
        "NUMBER_CHEMICAL_SHIFT_TYPE = 19\n",
        "\n",
        "def one_hot(cs):\n",
        "  '''\n",
        "  This function encodes the resnames so that there are now 4 columns \n",
        "  '''\n",
        "  one_hot = pd.get_dummies(cs['resname'])\n",
        "  cs = cs.join(one_hot)\n",
        "  return(cs)\n",
        "\n",
        "def get_cs_all(cs_all, id = \"2KOC\"):\n",
        "  '''    \n",
        "    This function gets chemical shifts for a particular RNA. \n",
        "    Assumes each RNA has a unique id  \n",
        "  '''\n",
        "  return(cs_all[(cs_all.id == id)])\n",
        "\n",
        "def get_cs_residues(cs_i, resid, dummy = 0):\n",
        "  '''    \n",
        "    This function return an array containing the chemical shifts for a particular residues in an RNA.    \n",
        "  '''\n",
        "  cs_tmp = cs_i[(cs_i.resid == resid)].drop(['id', 'resid', 'resname', 'ADE', 'CYT', 'GUA', 'URA', 'sasa-All-atoms', 'sasa-Total-Side',\n",
        "       'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All-polar'], axis=1)\n",
        "  info_tmp = cs_i[(cs_i.resid == resid)]\n",
        "  if (cs_tmp.shape[0] != 1):\n",
        "     return(dummy*np.ones(shape=(1, NUMBER_CHEMICAL_SHIFT_TYPE)))\n",
        "  else:\n",
        "     return(cs_tmp.values)\n",
        "    \n",
        "def get_resnames(cs_i, resid, dummy = \"UNK\"):\n",
        "  '''    \n",
        "    This function returns the residue name for specified residue (resid)\n",
        "  '''\n",
        "  cs_tmp = cs_i[(cs_i.resid == resid)]  \n",
        "  if (cs_tmp.shape[0] != 1):\n",
        "     return(dummy)\n",
        "  else:\n",
        "     return(cs_tmp['resname'].values[0])\n",
        "\n",
        "def get_cs_features(cs_i, resid, neighbors=1):\n",
        "  '''    \n",
        "  This function chemical shifts and resnames for residue (resid) and its neighbors        \n",
        "\n",
        "  '''\n",
        "  cs = []\n",
        "  resnames = []\n",
        "  for i in range(resid-neighbors, resid+neighbors+1):\n",
        "    cs.append(get_cs_residues(cs_i, i))\n",
        "    resnames.append(get_resnames(cs_i, i))\n",
        "  return(resnames, np.array(cs))\n",
        "\n",
        "def get_columns_names(neighbors = 3, chemical_shift_types = 19):\n",
        "  '''\n",
        "    \n",
        "    Helper function that writes out the required column names\n",
        "    \n",
        "  '''\n",
        "\n",
        "  columns = ['id', 'resname', 'resid', 'sasa-All-atoms', 'sasa-Total-Side',\n",
        "       'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All-polar', 'ADE', 'CYT', 'GUA', 'URA']\n",
        "  for i in range(0, neighbors*chemical_shift_types):\n",
        "    columns.append(i)\n",
        "  return(columns)\n",
        "\n",
        "def write_out_resname(neighbors=1):\n",
        "  '''\n",
        "  \n",
        "    Helper function that writes out the column names associated resnames for a given residue and its neighbors\n",
        "    \n",
        "  '''  \n",
        "  colnames = []\n",
        "  for i in range(1-neighbors-1, neighbors+1):\n",
        "    if i < 0: \n",
        "      colnames.append('R%s'%i)\n",
        "    elif i > 0: \n",
        "      colnames.append('R+%s'%i)\n",
        "    else: \n",
        "      colnames.append('R')\n",
        "  return(colnames)    \n",
        "\n",
        "\n",
        "def get_cs_features_rna(cs, neighbors=1, retain = ['id', 'sasa-All-atoms', 'sasa-Total-Side',\n",
        "       'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All-polar', 'resid', 'ADE', 'CYT', 'GUA', 'URA']):\n",
        "  '''    \n",
        "    This function generates the complete required data frame an RNA    \n",
        "  '''\n",
        "  all_features = []\n",
        "  all_resnames = []\n",
        "  for resid in cs['resid'].unique():\n",
        "    resnames, features = get_cs_features(cs, resid, neighbors)\n",
        "    all_features.append(features.flatten())\n",
        "    all_resnames.append(resnames)\n",
        "\n",
        "  all_resnames = pd.DataFrame(all_resnames, dtype='object', columns = write_out_resname(neighbors))\n",
        "  all_features = pd.DataFrame(all_features, dtype='object')\n",
        "  info = pd.DataFrame(cs[retain].values, dtype='object', columns = retain)\n",
        "  return(pd.concat([info, all_resnames, all_features], axis=1))\n",
        "\n",
        "def create_training_testing(cs, leave_out = \"2KOC\", target_name = ['sasa-All-atoms', 'sasa-Total-Side',\n",
        "       'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All-polar'], neighbors = 2, drop_names = ['id', 'sasa-All-atoms', 'sasa-Total-Side',\n",
        "       'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All-polar', 'resid']):\n",
        "  '''    \n",
        "    This function creates a training and testing set using leave one out    \n",
        "  '''\n",
        "  \n",
        "  # drop extraneous data  \n",
        "  drop_names = drop_names + list(write_out_resname(neighbors))  \n",
        "  \n",
        "  # does not contain leave_out\n",
        "  train = cs[(cs.id != leave_out)]\n",
        "  trainX = train.drop(drop_names, axis=1)\n",
        "  trainy = train[target_name]\n",
        " \n",
        "  # only contains leave_out\n",
        "  test = cs[(cs.id == leave_out)]\n",
        "  testX = test.drop(drop_names, axis=1)\n",
        "  testy = test[target_name]\n",
        "  \n",
        "  # return training and testing data\n",
        "  return(trainX.values, trainy.values, testX.values, testy.values)\n",
        "\n",
        "def get_cs_features_rna_all(cs, neighbors = 2):  \n",
        "  '''    \n",
        "    This [should] function generate a pandas dataframe containing training data for all RNAs\n",
        "    Each row in the data frame should contain the stacking and chemical shifts for given residue and neighbors in a given RNA.\n",
        "    Use the function above to write function\n",
        "    \n",
        "  '''  \n",
        "  # Start: your code\n",
        "  \n",
        "  cs_new = pd.DataFrame()\n",
        "  \n",
        "  for id in c.id.unique():\n",
        "    cs_id = get_cs_all(cs,id)\n",
        "    cs_new = pd.concat([cs_new,get_cs_features_rna(cs_id, neighbors)], axis = 0)\n",
        "  \n",
        "  \n",
        "  # End: your code\n",
        "  return(cs_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH6UzD_wgEVn",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIbjRYsdS1qK",
        "colab_type": "text"
      },
      "source": [
        "Download and prepare chemical shift data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpV715JKS5Yn",
        "colab_type": "code",
        "outputId": "23df9e62-6ebb-466f-a28e-4add46e033ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")   \n",
        "\n",
        "# load initial data\n",
        "url=\"https://drive.google.com/uc?id=1e-SHtWDtg4mD_th3_4Jmq9r1iiQC32wT\"\n",
        "s=requests.get(url).content\n",
        "c=pd.read_csv(io.StringIO(s.decode('utf-8')), sep=' ')\n",
        "print(\"[INFO]: loaded data\")\n",
        "\n",
        "# Drop extraneous columns that are unneeded for prediction\n",
        "c = c.drop(['Unnamed: 0','base_pairing', 'orientation', 'sugar_puckering', 'pseudoknot', 'stacking'], axis = 1)\n",
        "\n",
        "# One-hot encode the resname data so it can be used in the model\n",
        "c_new = one_hot(c)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: loaded data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on5p8uWlQOUw",
        "colab_type": "text"
      },
      "source": [
        "Download and prepare SASA datasest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKHxC4q0H4GY",
        "colab_type": "code",
        "outputId": "68ef5d71-eae7-4c8a-9be4-3011c7aff110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")   \n",
        "\n",
        "# load initial data\n",
        "url=\"https://drive.google.com/uc?export=download&id=1hn5uEIcP2sAwyTLeoGXNMdLm61T_Ew7c\"\n",
        "s=requests.get(url).content\n",
        "c_sasa=pd.read_csv(io.StringIO(s.decode('utf-8')), sep=' ')\n",
        "print(\"[INFO]: loaded data\")\n",
        "\n",
        "# The SASA file contains 5 extra rows, so they must be deleted\n",
        "c_sasa = c_sasa.drop(c_sasa.index[[235, 1254, 1280, 1947, 2436, 2476]])\n",
        "c_sasa = c_sasa.reset_index(drop=True)\n",
        "\n",
        "# remove redundant columns\n",
        "c_sasa = c_sasa.drop(['id', 'resid','resname'], axis = 1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: loaded data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNjYCrVAcrs5",
        "colab_type": "text"
      },
      "source": [
        "Combine chemical shifts and sasa datasets together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXQngexRQVOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_concat = pd.concat([c_new, c_sasa], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMli0kasc7zB",
        "colab_type": "text"
      },
      "source": [
        "Prepare the new dataset to contain neighbor chemical shifts as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMpN2f46a0BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The ideal number of neighbors found in prior assignment was Neighbors = 4\n",
        "NEIGHBORS = 4\n",
        "cs_all = get_cs_features_rna_all(c_concat, neighbors = NEIGHBORS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KswTqX7RgTj4",
        "colab_type": "text"
      },
      "source": [
        "## Determine best hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vek0ImEZdT7Y",
        "colab_type": "text"
      },
      "source": [
        "We need to identify some good hyperparameters to use. Below, we set up a distribution space that will be used to search through."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qng8SzSvgRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameter space distribution allows for random range of integers\n",
        "min_size, max_size = 10, 100\n",
        "parameter_space_distribution = {\n",
        "    'hidden_layer_sizes': [(sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': sp_expon(scale=.01),\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "    'learning_rate_init': sp_expon(scale=.01),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNTAvhICde-P",
        "colab_type": "text"
      },
      "source": [
        "Now, we set up the training and testing datasets so that we can use `RandomizedSearchCV` to search the parameter space for a suitable set of hyperparameters.\n",
        "\n",
        "Note: Sometimes the random search results in an error due to some value being `NaN`. We adjusted the parameter space distribution until we were able to search through 20 folds without any error occuring. When running again, we can't guarantee that the error won't occur at some point. After a few tries, you should be able to eventually get a set of best parameters without any error occuring.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJd0LtbKqVCb",
        "colab_type": "code",
        "outputId": "82f117a5-0828-4be2-f020-8451ff31904e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# Seperate data into training and testing set\n",
        "trainX, trainy, testX, testy = create_training_testing(cs_all, leave_out = '1A60', neighbors = NEIGHBORS)\n",
        "print(\"[INFO]: created training and testing data structures\")\n",
        "\n",
        "# setup scaler and transform the X data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(trainX)\n",
        "trainX_scaled = scaler.transform(trainX)\n",
        "testX_scaled = scaler.transform(testX)\n",
        "print(\"[INFO]: scaled the features\")\n",
        "\n",
        "# set the model to be a multilayer perceptron regressor\n",
        "model = MLPRegressor()\n",
        "\n",
        "# Random search for best hyperparameter\n",
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(model, param_distributions=parameter_space_distribution, n_iter=n_iter_search, cv=2, verbose = 1)\n",
        "random_search.fit(trainX_scaled, trainy)\n",
        "random_search.best_params_"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n",
            "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  1.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'tanh',\n",
              " 'alpha': 0.005654169847189843,\n",
              " 'hidden_layer_sizes': (181,),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.006401053696231486,\n",
              " 'solver': 'adam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhGPvobTej1I",
        "colab_type": "text"
      },
      "source": [
        "Good parameters found previously were:\n",
        "```\n",
        "activation = 'tanh',\n",
        "alpha = 0.005654169847189843,\n",
        "hidden_layer_sizes = (181,),\n",
        "learning_rate = 'constant',\n",
        "learning_rate_init = 0.006401053696231486,\n",
        "solver = 'adam'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qMjc-nxgZq_",
        "colab_type": "text"
      },
      "source": [
        "## Train the data and calculate mean R2 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8hKRq27etGY",
        "colab_type": "text"
      },
      "source": [
        "The next section of code is where the fit and evaluation occur. For each loop, one of the IDs are left out to use as a testing set. This will loop through every ID and store the r2 scores of each loop into an `r2scores` array. At the end, it calculates the mean r2 scores.\n",
        "\n",
        "In order to ensure that the same scores are reached every time the code is run, we don't use `random_search` explicitly here. Instead, we use a set of hyperparameters that were found previously from a `RandomizedSearchCV` run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qq4AE5uwOW-",
        "colab_type": "code",
        "outputId": "9852f9bc-40ab-49f7-d946-f70a6504216f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mlp_R2 = []\n",
        "mtl_R2 = []\n",
        "knr_R2 = []\n",
        "gpr_R2 = []\n",
        "dtr_R2 = []\n",
        "combined_R2 = []\n",
        "comb_red_R2 = []\n",
        "\n",
        "id_array = cs_all.id.unique()\n",
        "for id in id_array:\n",
        "  trainX, trainy, testX, testy = create_training_testing(cs_all, leave_out = id, neighbors = NEIGHBORS)\n",
        "  print(\"[INFO]: created training and testing data structures for \" + id)\n",
        "\n",
        "  # setup scaler\n",
        "  scaler.fit(trainX)\n",
        "  trainX_scaled = scaler.transform(trainX)\n",
        "  testX_scaled = scaler.transform(testX)\n",
        "  print(\"[INFO]: scaled the features\")\n",
        "\n",
        "  # Fit tthe multilayer perceptron model\n",
        "  mlp_model = sklearn.neural_network.MLPRegressor(activation = 'tanh',\n",
        "                                                  alpha = 0.005654169847189843,\n",
        "                                                  hidden_layer_sizes = (181,),\n",
        "                                                  learning_rate = 'constant',\n",
        "                                                  learning_rate_init = 0.006401053696231486,\n",
        "                                                  solver = 'adam')\n",
        "  mlp_model.fit(trainX_scaled,trainy)\n",
        "\n",
        "  # fit the multitask lasso model\n",
        "  mtl_model = MultiTaskLasso()\n",
        "  mtl_model.fit(trainX_scaled,trainy)\n",
        "\n",
        "  # fit the K Neighbors model\n",
        "  knr_model = KNeighborsRegressor()\n",
        "  knr_model.fit(trainX_scaled,trainy)\n",
        "\n",
        "  # fit the gaussian process model\n",
        "  gpr_model = GaussianProcessRegressor()\n",
        "  gpr_model.fit(trainX_scaled,trainy)\n",
        "\n",
        "  # fit the decision tree model\n",
        "  dtr_model = DecisionTreeRegressor()\n",
        "  dtr_model.fit(trainX_scaled,trainy)\n",
        "\n",
        "  # Predict y values\n",
        "  y_true = testy \n",
        "  mlp_ypred = mlp_model.predict(testX_scaled)\n",
        "  mtl_ypred = mtl_model.predict(testX_scaled)\n",
        "  knr_ypred = knr_model.predict(testX_scaled)\n",
        "  gpr_ypred = gpr_model.predict(testX_scaled)\n",
        "  dtr_ypred = dtr_model.predict(testX_scaled)\n",
        "\n",
        "  # Combined model averages over baseline model y-value predictions\n",
        "  combined_ypred = np.mean([mlp_ypred, mtl_ypred, knr_ypred, gpr_ypred, dtr_ypred],axis=0)\n",
        "  comb_red_ypred = np.mean([mlp_ypred, mtl_ypred, knr_ypred],axis=0)\n",
        "\n",
        "  # Record R2 scores for each of the models\n",
        "  mlp_R2.append(sklearn.metrics.r2_score(y_true,mlp_ypred))\n",
        "  mtl_R2.append(sklearn.metrics.r2_score(y_true,mtl_ypred))\n",
        "  knr_R2.append(sklearn.metrics.r2_score(y_true,knr_ypred))\n",
        "  gpr_R2.append(sklearn.metrics.r2_score(y_true,gpr_ypred))\n",
        "  dtr_R2.append(sklearn.metrics.r2_score(y_true,dtr_ypred))\n",
        "  combined_R2.append(sklearn.metrics.r2_score(y_true,combined_ypred))\n",
        "  comb_red_R2.append(sklearn.metrics.r2_score(y_true,comb_red_ypred))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: created training and testing data structures for 1A60\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1HWQ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1JO7\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1KKA\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1L1W\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1LC6\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1LDZ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1MFY\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1NA2\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1NC0\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1OW9\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1PJY\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1Q75\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1R7W\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1R7Z\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1SCL\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1SY4\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1SYZ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1UUU\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1XHP\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1YMO\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1YSV\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1Z2J\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1Z30\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 1ZC5\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 28SP\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 28SR\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2F87\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2FDT\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2GVO\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2JR4\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2JWV\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2JYM\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2K66\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2KEZ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2KF0\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2KOC\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2KXM\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2KZL\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2L3E\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2L5Z\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2L6I\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2L8H\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LAC\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LBJ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LBK\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LBL\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LDL\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LDT\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LHP\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LI4\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LJJ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LK3\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LP9\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LPA\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LPS\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LQZ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LUB\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LUN\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2LV0\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2M12\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2M21\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2M22\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2M4W\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2M5U\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2M8K\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2MEQ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2MFD\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2MHI\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2MIS\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2MNC\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2MTJ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2MXL\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N2O\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N2P\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N3Q\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N3R\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N4L\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N6S\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N6T\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N6W\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2N6X\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2NBY\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2NBZ\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2NC0\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2NC1\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2NCI\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2O33\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2QH2\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2QH3\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2QH4\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2RVO\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 2Y95\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 4A4S\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 4A4T\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 4A4U\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5A17\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5A18\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5IEM\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5KQE\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5UF3\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5UZT\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5V16\n",
            "[INFO]: scaled the features\n",
            "[INFO]: created training and testing data structures for 5WQ1\n",
            "[INFO]: scaled the features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3PsyZS9bOgj",
        "colab_type": "code",
        "outputId": "037c91f2-0def-49d2-9ca2-42a4651c9778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "print('The mean scores are: ')\n",
        "print('mlp:')\n",
        "print(np.mean(mlp_R2))\n",
        "print('mtl:')\n",
        "print(np.mean(mtl_R2))\n",
        "print('knr:')\n",
        "print(np.mean(knr_R2))\n",
        "print('gpr:')\n",
        "print(np.mean(gpr_R2))\n",
        "print('dtr:')\n",
        "print(np.mean(dtr_R2))\n",
        "print('combined:')\n",
        "print(np.mean(combined_R2))\n",
        "print('ccombined_r:')\n",
        "print(np.mean(comb_red_R2))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean scores are: \n",
            "mlp:\n",
            "0.3288263286289028\n",
            "mtl:\n",
            "0.36400947047729004\n",
            "knr:\n",
            "0.3624662505776763\n",
            "gpr:\n",
            "-22.572444930234152\n",
            "dtr:\n",
            "-0.3154695174488031\n",
            "combined:\n",
            "-0.547006942659105\n",
            "ccombined_r:\n",
            "0.44184868311959835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIAuIPa2gh4a",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion:\n",
        "\n",
        "It is clear that the Gaussian Process Regressor and the Decision Tree Regressor do not work at all since their R2 scores are negative. The Combined Model ends up with a negative score as well because of this. The Reduced Combined Model is much more promising. Its value is consistently higher than any of the three models which it is comprised of. While the value is still low, it shows promise for future consideration.\n",
        "\n",
        "Trial 1:\n",
        "\n",
        "mlp:\n",
        "0.3317651274871274\n",
        "\n",
        "mtl:\n",
        "0.36400947047729004\n",
        "\n",
        "knr:\n",
        "0.3624662505776763\n",
        "\n",
        "gpr:\n",
        "-22.572444930234152\n",
        "\n",
        "dtr:\n",
        "-0.25904090630243554\n",
        "\n",
        "combined:\n",
        "-0.5436059594629794\n",
        "\n",
        "ccombined_r:\n",
        "0.443197379356901\n",
        "\n",
        "Trial 2:\n",
        "\n",
        "mlp:\n",
        "0.3422078372361123\n",
        "\n",
        "mtl:\n",
        "0.36400947047729004\n",
        "\n",
        "knr:\n",
        "0.3624662505776763\n",
        "\n",
        "gpr:\n",
        "-22.572444930234152\n",
        "\n",
        "dtr:\n",
        "-0.28796273804092515\n",
        "\n",
        "combined:\n",
        "-0.5379015072399548\n",
        "\n",
        "ccombined_r:\n",
        "0.4462537479044602\n",
        "\n",
        "Trial 3:\n",
        "\n",
        "mlp:\n",
        "0.3288263286289028\n",
        "\n",
        "mtl:\n",
        "0.36400947047729004\n",
        "\n",
        "knr:\n",
        "0.3624662505776763\n",
        "\n",
        "gpr:\n",
        "-22.572444930234152\n",
        "\n",
        "dtr:\n",
        "-0.3154695174488031\n",
        "\n",
        "combined:\n",
        "-0.547006942659105\n",
        "\n",
        "ccombined_r:\n",
        "0.44184868311959835"
      ]
    }
  ]
}